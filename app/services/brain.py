"""
Servi√ßo de Intelig√™ncia Artificial H√≠brido.
Ouvido: Groq (Whisper)
C√©rebro: Groq (llama-3.3-70b-versatile)
Mem√≥ria: Persist√™ncia em Arquivo JSON (Resistente a reinicializa√ß√µes do Docker)
"""
import pathlib
import os
import json
from openai import AsyncOpenAI
from app.config import settings
from app.utils.logger import setup_logger
from app.utils.retry_handler import retry_with_backoff, get_retryable_exceptions
from .appointment import AppointmentService

logger = setup_logger(__name__)

class BrainService:
    """
    Gerenciador de racioc√≠nio, audi√ß√£o e mem√≥ria persistente.
    """

    # Prompt de Vendas
    SYSTEM_PROMPT = """
    Voc√™ √© o Alex, um SDR s√™nior e consultor da 'TechSolutions'.
    
    OBJETIVO PRINCIPAL:
    Conversar naturalmente com o lead para entender suas necessidades e, se fizer sentido, agendar uma reuni√£o.
    
    SERVI√áOS DA EMPRESA:
    - Desenvolvimento de software personalizado
    - Consultoria em tecnologia da informa√ß√£o
    - Seguran√ßa cibern√©tica
    - An√°lise de dados e intelig√™ncia de neg√≥cios
    - Automa√ß√£o de processos
    - Gest√£o de projetos e inova√ß√£o digital
    
    DIRETRIZES IMPORTANTES:
    1. Responda SOMENTE perguntas relacionadas aos servi√ßos da TechSolutions.
    2. Se o usu√°rio perguntar sobre algo fora do escopo da TechSolutions, informe educadamente que voc√™ s√≥ pode ajudar com assuntos relacionados √† empresa.
    3. Responda de forma fluida e humana (varie o vocabul√°rio, evite repetir v√≠cios de linguagem como 't√° bom' em toda frase).
    4. Seja conciso, mas entregue valor (respostas ideais entre 1 a 3 frases).
    5. Use tom profissional mas acolhedor.
    6. NUNCA use emojis.
    7. Sempre mantenha a conversa viva com uma pergunta relevante no final.
    8. Jamais responda perguntas sobre outros assuntos (hist√≥ria, geografia, ci√™ncia, etc.)
    """

    def __init__(self):
        # 1. Configura o C√âREBRO (Texto -> Texto)
        try:
            self.client_brain = AsyncOpenAI(
                api_key=settings.openai_api_key,
                base_url=settings.openai_base_url
            )
            self.model_brain = settings.openai_model
            logger.info(f"üß† C√©rebro conectado: {self.model_brain}")
        except Exception as e:
            logger.critical(f"Falha ao iniciar C√©rebro: {e}")
            raise

        # 2. Configura o OUVIDO (√Åudio -> Texto)
        self.groq_api_key = os.getenv("GROQ_API_KEY")
        
        if self.groq_api_key:
            self.client_ear = AsyncOpenAI(
                api_key=self.groq_api_key,
                base_url="https://api.groq.com/openai/v1"
            )
            logger.info("üëÇ Ouvido ativado: Whisper via Groq.")
        else:
            self.client_ear = None
            logger.warning("‚ö†Ô∏è Chave GROQ_API_KEY n√£o encontrada. Modo surdo.")

        # 3. Configura o SERVI√áO DE AGENDAMENTO
        self.appointment_service = AppointmentService()
        
        # --- MEM√ìRIA PERSISTENTE (JSON) ---
        # Carrega o hist√≥rico do arquivo ao iniciar
        self.history_file = pathlib.Path("chat_history.json")
        self.sessions = self._load_memory()

    def _load_memory(self) -> dict:
        """Carrega hist√≥rico do disco se existir"""
        if self.history_file.exists():
            try:
                with open(self.history_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    logger.info(f"üìÇ Mem√≥ria carregada: {len(data)} conversas recuperadas.")
                    return data
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Erro ao carregar mem√≥ria (iniciando vazia): {e}")
        return {}

    def _save_memory(self):
        """Salva hist√≥rico no disco"""
        try:
            with open(self.history_file, "w", encoding="utf-8") as f:
                json.dump(self.sessions, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar mem√≥ria: {e}")

    def _update_memory(self, remote_jid: str, role: str, content: str):
        """Atualiza mem√≥ria e salva no disco imediatamente"""
        if remote_jid not in self.sessions:
            self.sessions[remote_jid] = []
        
        # Adiciona nova mensagem ao hist√≥rico
        self.sessions[remote_jid].append({"role": role, "content": content})
        
        # Mant√©m apenas as √∫ltimas 20 intera√ß√µes (Janela de Contexto)
        if len(self.sessions[remote_jid]) > 20:
            self.sessions[remote_jid] = self.sessions[remote_jid][-20:]
            
        # Persiste a altera√ß√£o no arquivo
        self._save_memory()

    def _is_off_topic_request(self, user_text: str) -> bool:
        """
        Detecta se a mensagem do usu√°rio √© sobre um assunto fora do escopo da TechSolutions
        """
        user_text_lower = user_text.lower()
        
        # Palavras-chave comuns em perguntas fora do escopo
        off_topic_keywords = [
            # Perguntas gerais
            "quem foi", "quem descobriu", "por que o brasil", "hist√≥ria do brasil", 
            "quando foi", "o que foi", "como surgiu", "qual a origem",
            
            # Assuntos acad√™micos
            "mat√©ria de", "estudar ", "escola", "professor", "prova", "trabalho de ",
            
            # Assuntos pessoais n√£o relacionados ao neg√≥cio
            "namorar", "casar", "casamento", "filhos", "fam√≠lia", "relacionamento",
            
            # Assuntos n√£o empresariais
            "pol√≠tica", "elei√ß√£o", "governador", "prefeito", "presidente",
            
            # Assuntos n√£o relacionados √† tecnologia/neg√≥cios
            "culin√°ria", "receita", "comida", "filme", "m√∫sica", "esporte",
            
            # Perguntas sobre a pr√≥pria IA (se o usu√°rio mencionar que est√° sendo atendido por um bot)
            "voc√™ √© um bot", "voc√™ √© humano", "quem criou voc√™", "intelig√™ncia artificial",
        ]
        
        # Verifica se alguma palavra-chave est√° presente no texto
        for keyword in off_topic_keywords:
            if keyword in user_text_lower:
                return True
        
        # Verifica padr√µes de perguntas comuns fora do escopo
        question_patterns = [
            "quem foi ", "quem descobriu ", "quem inventou ", "quem criou ",
            "quando foi ", "como surgiu ", "qual a origem ", "de onde veio ",
            "o que √© ", "o que foi ", "historia de ", "hist√≥ria de "
        ]
        
        for pattern in question_patterns:
            if pattern in user_text_lower:
                return True
                
        return False

    def _generate_off_topic_response(self) -> str:
        """
        Gera uma resposta educada para quando o usu√°rio faz perguntas fora do escopo
        """
        responses = [
            "Desculpe, mas s√≥ posso ajudar com informa√ß√µes sobre os servi√ßos da TechSolutions. Posso te ajudar com algo relacionado √† tecnologia da informa√ß√£o, desenvolvimento de software, consultoria ou automa√ß√£o de processos?",
            "Essa pergunta est√° fora do meu campo de atua√ß√£o. Sou assistente da TechSolutions e posso te ajudar com nossos servi√ßos de tecnologia. Gostaria de saber mais sobre como podemos ajudar o seu neg√≥cio?",
            "Infelizmente n√£o posso responder sobre esse assunto. Estou aqui para apresentar os servi√ßos da TechSolutions. Tem interesse em solu√ß√µes de TI, consultoria ou automa√ß√£o?",
            "S√≥ posso fornecer informa√ß√µes sobre os servi√ßos da TechSolutions. Somos especializados em desenvolvimento de software, consultoria em TI, seguran√ßa cibern√©tica e automa√ß√£o de processos. Gostaria de saber mais sobre algum desses servi√ßos?"
        ]
        
        # Retorna uma resposta aleat√≥ria para variar
        import random
        return random.choice(responses)

    @retry_with_backoff(
        max_retries=3,
        base_delay=1.0,
        max_delay=30.0,
        backoff_factor=2.0,
        exceptions=get_retryable_exceptions() + (Exception,)
    )
    async def transcribe_audio(self, audio_path: str) -> str:
        """
        Transcreve o √°udio usando Groq Whisper.
        """
        if not self.client_ear:
            logger.warning("Simulando audi√ß√£o (Sem chave Groq)")
            return "Ol√°, gostaria de saber mais."

        try:
            path_obj = pathlib.Path(audio_path)
            if not path_obj.exists():
                logger.error(f"Arquivo de √°udio n√£o existe: {audio_path}")
                return ""

            with open(path_obj, "rb") as audio_file:
                transcription = await self.client_ear.audio.transcriptions.create(
                    file=audio_file,
                    model="whisper-large-v3", 
                    response_format="text",
                    language="pt" 
                )
            
            text_result = str(transcription).strip()
            logger.info(f"üó£Ô∏è Transcri√ß√£o Real: {text_result}")
            return text_result

        except Exception as e:
            logger.error(f"‚ùå Erro na transcri√ß√£o (Groq): {e}")
            return ""

    @retry_with_backoff(
        max_retries=3,
        base_delay=1.0,
        max_delay=30.0,
        backoff_factor=2.0,
        exceptions=get_retryable_exceptions() + (Exception,)
    )
    async def process_audio_and_respond(self, audio_path: str | pathlib.Path, remote_jid: str) -> str:
        """
        Pipeline: Ouvir -> Carregar Contexto -> Pensar -> Salvar Contexto
        """
        try:
            # 1. Ouvir (Transcri√ß√£o)
            user_text = await self.transcribe_audio(str(audio_path))
            
            if not user_text or len(user_text) < 2: 
                return "Oi, n√£o consegui te ouvir direito. Pode mandar de novo?"

            # 2. Verificar se a solicita√ß√£o est√° fora do escopo antes de processar pela IA
            if self._is_off_topic_request(user_text):
                off_topic_response = self._generate_off_topic_response()
                self._update_memory(remote_jid, "assistant", off_topic_response)
                logger.info(f"üéØ Resposta fora do escopo para {remote_jid}: {off_topic_response}")
                return off_topic_response

            # 3. Verificar inten√ß√£o de agendamento antes de processar pela IA
            scheduling_response = await self.appointment_service.handle_appointment_request(type('obj', (object,), {'body': user_text})())
            if scheduling_response:
                # Adiciona resposta de agendamento ao hist√≥rico e retorna
                self._update_memory(remote_jid, "assistant", scheduling_response)
                logger.info(f"üìÖ Resposta de agendamento enviada: {scheduling_response}")
                # Retorna a resposta com um prefixo especial para indicar que √© uma resposta de agendamento
                return f"[SCHEDULING_RESPONSE]{scheduling_response}"

            # 4. Atualizar Mem√≥ria com a fala do usu√°rio
            self._update_memory(remote_jid, "user", user_text)

            # 5. Construir Contexto para a IA
            messages_payload = [{"role": "system", "content": self.SYSTEM_PROMPT}]
            
            if remote_jid in self.sessions:
                messages_payload.extend(self.sessions[remote_jid])

            # 6. Pensar (Envia hist√≥rico completo)
            response = await self.client_brain.chat.completions.create(
                model=self.model_brain,
                messages=messages_payload,
                temperature=0.6,
                max_tokens=150
            )

            reply = response.choices[0].message.content
            
            # Limpeza da resposta
            clean_reply = reply.strip().replace('"', '').replace("*", "")
            
            # 7. Atualizar Mem√≥ria com a resposta do Bot
            self._update_memory(remote_jid, "assistant", clean_reply)
            
            logger.info(f"üß† C√©rebro Respondeu: {clean_reply}")
            return clean_reply

        except Exception as e:
            logger.error(f"‚ùå Erro no c√©rebro: {e}", exc_info=True)
            return "Oi! Tive um problema t√©cnico. Pode repetir o √°udio?"

    @retry_with_backoff(
        max_retries=3,
        base_delay=1.0,
        max_delay=30.0,
        backoff_factor=2.0,
        exceptions=get_retryable_exceptions() + (Exception,)
    )
    async def process_text_and_respond(self, user_text: str, remote_jid: str) -> str:
        """
        Processa mensagem de texto diretamente, sem necessidade de transcri√ß√£o.
        """
        try:
            if not user_text or len(user_text) < 2: 
                return "Oi, n√£o consegui entender direito. Pode repetir?"

            # 1. Verificar se a solicita√ß√£o est√° fora do escopo antes de processar pela IA
            if self._is_off_topic_request(user_text):
                off_topic_response = self._generate_off_topic_response()
                self._update_memory(remote_jid, "assistant", off_topic_response)
                logger.info(f"üéØ Resposta fora do escopo para {remote_jid}: {off_topic_response}")
                return off_topic_response

            # 2. Verificar inten√ß√£o de agendamento antes de processar pela IA
            scheduling_response = await self.appointment_service.handle_appointment_request(type('obj', (object,), {'body': user_text})())
            if scheduling_response:
                # Adiciona resposta de agendamento ao hist√≥rico e retorna
                self._update_memory(remote_jid, "assistant", scheduling_response)
                logger.info(f"üìÖ Resposta de agendamento enviada: {scheduling_response}")
                # Retorna a resposta com um prefixo especial para indicar que √© uma resposta de agendamento
                return f"[SCHEDULING_RESPONSE]{scheduling_response}"

            # 3. Atualizar Mem√≥ria com a mensagem do usu√°rio
            self._update_memory(remote_jid, "user", user_text)

            # 4. Construir Contexto para a IA
            messages_payload = [{"role": "system", "content": self.SYSTEM_PROMPT}]
            
            if remote_jid in self.sessions:
                messages_payload.extend(self.sessions[remote_jid])

            # 5. Pensar (Envia hist√≥rico completo)
            response = await self.client_brain.chat.completions.create(
                model=self.model_brain,
                messages=messages_payload,
                temperature=0.6,
                max_tokens=150
            )

            reply = response.choices[0].message.content
            
            # Limpeza da resposta
            clean_reply = reply.strip().replace('"', '').replace("*", "")
            
            # 6. Atualizar Mem√≥ria com a resposta do Bot
            self._update_memory(remote_jid, "assistant", clean_reply)
            
            logger.info(f"üß† C√©rebro Respondeu (texto): {clean_reply}")
            return clean_reply

        except Exception as e:
            logger.error(f"‚ùå Erro no c√©rebro (texto): {e}", exc_info=True)
            return "Oi! Tive um problema t√©cnico. Pode repetir a mensagem?"

# Singleton
brain_service = BrainService()